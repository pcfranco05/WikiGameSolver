{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd41da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80562fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinksFromTextBS(start_article):\n",
    "    page_titles = []\n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'parse',\n",
    "        'page': start_article,\n",
    "        'format': 'json',\n",
    "        'prop': 'text',\n",
    "        'redirects': ''\n",
    "    }\n",
    "\n",
    "    filter_sections = ['See also',\n",
    "                       'References',\n",
    "                       'External links',\n",
    "                       'Further reading',\n",
    "                       'Notes']\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "\n",
    "    raw_html = data['parse']['text']['*']\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    # Get all the section names\n",
    "    allSections = soup.find_all(class_='mw-headline')\n",
    "\n",
    "    sectionNames = []\n",
    "    for section in allSections:\n",
    "        sectionNames.append(section.get_text())\n",
    "\n",
    "    sectionNames = [x for x in sectionNames if x not in filter_sections]\n",
    "\n",
    "    # Get links from summary section up to the Table of Contents\n",
    "    target = soup.find(class_='mw-parser-output')\n",
    "    if target:\n",
    "        for sib in target.find_all_next():\n",
    "            # Only look in current section, end if hitting next section\n",
    "            # print(sib)\n",
    "            if sib.name == \"h2\":\n",
    "                break\n",
    "            elif 'title' in sib.attrs and 'Edit this' in sib.attrs['title']:\n",
    "                # Don't include the hrefs to edit the pages\n",
    "                continue\n",
    "            else:\n",
    "                # Check if href contains internal /wiki/ path\n",
    "                check1 = 'href' in sib.attrs and \\\n",
    "                         '/wiki/' in sib.attrs['href']\n",
    "                # Check if tag contains class mw-redirect\n",
    "                check2 = 'class' in sib.attrs and \\\n",
    "                         'mw-redirect' in sib.attrs['class']\n",
    "                if (check1 or check2) and 'title' in sib.attrs and 'wiktionary' not in sib.attrs['title']:\n",
    "                    page_titles.append(sib.attrs['title'])\n",
    "\n",
    "    # Get all the links from each of the relevant sections\n",
    "    for thisSection in sectionNames:\n",
    "        # print('==--------' + thisSection + '--------==')\n",
    "        target = soup.find(class_='mw-headline', id=thisSection.replace(' ', '_'))\n",
    "        if target:\n",
    "            for sib in target.find_all_next():\n",
    "                # Only look in current section, end if hitting next section\n",
    "                # print(sib)\n",
    "                if sib.name == \"h2\":\n",
    "                    break\n",
    "                else:\n",
    "                    # Check if href contains internal /wiki/ path\n",
    "                    check1 = 'href' in sib.attrs and \\\n",
    "                    '/wiki/' in sib.attrs['href']\n",
    "                    # Check if tag contains class mw-redirect\n",
    "                    check2 = 'class' in sib.attrs and \\\n",
    "                        'mw-redirect' in sib.attrs['class']\n",
    "                    if (check1 or check2) and 'title' in sib.attrs:\n",
    "                        page_titles.append(sib.attrs['title'])\n",
    "    # Unique pages only\n",
    "    page_titles = list(set(page_titles))\n",
    "    return page_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe434a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['List of largest snakes', 'The Naturalist on the River Amazons', 'Floodplain', 'Pororoca', 'Agriculture', 'Pará', 'Area (journal)', 'Source of the Amazon River', 'Cretaceous', 'Conquistador', 'Pleistocene', 'Iquitos', 'Biodiversity of Colombia', 'Lake Junin', 'Uarini River', 'Paru River', 'El Dorado', 'Alfred Russel Wallace', 'Parintins', 'Cacao bean', 'International Union for Conservation of Nature', 'The Daily Telegraph', 'Silver arowana', 'Carhuasanta', 'Distributary', 'Sea level', 'Undular bore', 'Sarmatians', 'Georg von Langsdorff', 'Terra preta', 'Breves, Pará', 'Piranha', 'Morona', 'Orinoco River', 'Ecuador', 'Salinity', 'Juruena River', 'Nhamundá River', 'Tidal bore', 'Tabatinga', 'Giant otter', 'Getúlio Vargas', 'Spanish Empire', 'Carl Friedrich Philipp von Martius', 'Amazon Theatre', 'Jutaí River', 'Casiquiare canal', 'River bifurcation', 'Guajará River (Amazon)', 'Vila Nova River', 'Nevado Mismi', 'Apurímac River', 'Monte Alegre, Pará', 'Bull shark', 'Natural rubber', 'Caqueta River', 'Colonization', 'Javary River', 'Formative stage', 'Badajós River', 'Lake Titicaca', 'Irineu Evangelista de Sousa', 'Manaus', 'White Brazilian', 'Chambira River', 'List of river systems by length', 'Brazilian Institute of Geography and Statistics', 'Amazonas (Brazilian state)', 'Dry season', 'Charles Marie de La Condamine', 'Santarém, Brazil', 'Coastline paradox', 'Matapi River', 'Help:IPA/English', 'Stream channel', 'Dolphin', 'Paraná do Urariá', 'Quito', 'Meeting of Waters', 'Sediment', 'Scholarly peer review', 'Typhus', 'Francisco de Orellana', 'Hammock', 'Purus River', 'Pongo de Manseriche', 'Jupati River', 'Almeirim, Pará', 's:1911 Encyclopædia Britannica/Amazon', 'Anajás River', 'Igapó-Açu River', 'Arapaima', 'Napo River', 'Rio Negro (Amazon)', 'Madeira River', 'Coari River', 'Wet season', 'Iquitos, Peru', 'Metagenomics', 'List of butterflies of the Amazon River basin and the Andes', 'Royal Geographical Society', 'Ancient Greece', 'Wikipedia:Manual of Style/Words to watch', 'Apurimac River', 'Ecosystem collapse', 'Tambaqui', 'Evangelism', 'Allpahuayo-Mishana National Reserve', 'Marajó', 'Amazon basin', 'Gondwana', 'Curuá River (Amazon River tributary)', 'Amazon river dolphin', 'Jari River', 'Tributary', 'Freshwater', 'Caiman', 'Guaporé River', 'Neotropical fishes', 'Vicente Yáñez Pinzón', 'Mercantile', 'Arequipa Region', 'Resin', 'Pedro II of Brazil', 'Madre de Dios River', 'Steamboat', 'Headwaters', 'Confluence', 'Johann Baptist von Spix', 'Osteoglossum bicirrhosum', 'Chiefdom', 'Betaproteobacteria', 'National Institute for Space Research', 'Pastaza River', 'Uatumã River', 'Solimões', 'Cabanagem', 'Category:Flora of the Amazon', 'Alexander von Humboldt', 'Freshwater fish', 'Atuá River', 'Belém', 'Catfish', 'Lumber', 'Piorini River', 'River mouth', 'Coca River', 'Juruá River', 'Vaupés River', 'Malaria', 'List of rivers by length', 'Tributaries', 'Paracauari River', 'Electric eel', 'Huallaga River', 'Guainía River', 'Leticia, Amazonas', 'Candirú', 'Huánuco', 'International Scale of River Difficulty', 'Tefé River', 'Timeline of Amazon history', 'Solimões River', 'Gurupá', 'Fur', 'Grão-Pará Province', '5th parallel north', 'Amazônia National Park', 'Karma', 'Discharge (hydrology)', 'Characin', 'Spanish language', 'Santarém, Pará', 'Gymnotiformes', 'Nile', 'Rio Preto da Eva', 'Gammaproteobacteria', 'Amazons', 'Guamá River', 'Glacial period', 'Draft (hull)', 'Bridge', 'Trichomycteridae', 'Hamza River', 'Lope de Aguirre', 'Pracuúba', 'Amazonian manatee', 'Ticuna', 'Andes Mountains', 'Animal echolocation', 'Tigre River', 'Branco River', 'George Earl Church', 'Andean civilizations', 'Itacoatiara, Amazonas', 'Itaya River', 'Indo-Iranian languages', 'Mamiá River (Amazonas)', 'Main stem', 'Kawahíb people', 'Tonantins River', 'Óbidos, Brazil', 'Miocene', 'Samuel Fritz', 'Arari River', 'António Raposo Tavares', 'Richard Spruce', 'Amapá', 'Ferry', 'Óbidos, Pará', 'Aguarico River', 'Araguari River (Amapá)', 'Terrace (agriculture)', 'Freshwater Angelfish', 'Anavilhanas National Park', 'Marajoara culture', 'Upper Amazon', 'American English', 'Category:Fauna of the Amazon', 'Lima', 'Biodiversity', 'Alphaproteobacteria', 'Acará River', 'Tapajós', 'University of Chicago', 'Thermoproteota', 'Urubu River (Amazonas)', 'Hesychius of Alexandria', 'Putumayo River', 'Brazil', 'Tocantins River', 'Cinnamon', 'Araguaia River', 'Manacapuru River', 'Category:Birds of the Amazon Basin', 'Tambo River (Peru)', 'Pardo', 'Caquetá River', 'Curuçá', 'Pedreira River', 'Japurá River', 'Peru', 'Ethnonym', 'Orinoco', 'Amacayacu National Park', 'Nazca Plate', 'Sandstone', 'Nanay River', 'Trans-Amazonian Highway', 'Tupi language', 'Bay', 'Estuary', 'Archaeologist', 'Tectonic uplift', 'Greek mythology', 'Trombetas River', 'Urubamba River', 'Indigenous peoples of Brazil', 'Amazon rainforest', 'South American Plate', 'Tucuxi', 'Canumã River', 'Amazon Delta', 'Ucayali River', 'Xingu River', 'NASA', 'Marañón River', 'Munduruku', 'Neon tetra', 'Spanish conquistador', 'Tefé', 'Arequipa', 'Guayana Shield', 'Mantaro River', 'Kumakuma', 'Maicuru River', 'Wikipedia:Please clarify', 'Jandiatuba River', 'Gonzalo Pizarro', 'Arowana', 'Muisca economy', 'Sanskrit', 'Inca Empire', 'National Geographic', 'Curuá Una River', 'British English', 'Rubber boom', 'Colombia', 'Manacapuru', 'Actinomycetota', 'Pacific Ocean', 'La Canela', 'Enlarge', 'Drainage basin', 'Teles Pires', 'Manaus Iranduba Bridge', 'Anaconda', 'Public domain', 'Bandeirante', 'Macapá', 'Tapajós River', 'Brazil nut', 'Encyclopædia Britannica', 'Iranian peoples', 'Amazon delta', 'Atlantic Ocean', 'Puerto Francisco de Orellana', 'Encyclopædia Britannica Eleventh Edition', 'Freshwater swamp forest', '20th parallel south', 'Henry Walter Bates', 'Neon Tetra', 'BR-319', 'Porto Velho', 'Juruá', 'Iriri River', 'Jutai River', 'Pará River', 'Negro River (Amazon)', 'Boto', 'Andes', 'Muisca Confederation', 'Portuguese language', 'Category:Trees of the Amazon', 'Potamotrygonidae', 'Santarem, Brazil', 'South America', 'James S. Olson', 'Purús River', 'Urarina', 'River dolphin', 'Anabranch', 'Green anaconda', 'São Paulo', 'Trombetas', 'Scythians', 'Nauta', 'Feather', 'Lawriqucha', 'Ucayali', 'Pedro Teixeira', 'List of rivers by discharge']\n"
     ]
    }
   ],
   "source": [
    "# for the following\n",
    "# start_article = 'Amazon River'\n",
    "# end_article = 'Emotion'\n",
    "\n",
    "links = getLinksFromTextBS('Amazon River')\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3269081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Word Similarity Check\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# wordScore takes in the list of words from a wikipedia link and the name of the goal page.\n",
    "def wordScore(word1, word2, model, logs=0):\n",
    "    if model == 'bert':\n",
    "        model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    elif model == 'roberta':\n",
    "        model = SentenceTransformer('all-distilroberta-v1')\n",
    "    elif model == 'microsoftNet':\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    elif model == 'L12':\n",
    "        model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    elif model == 'L6':\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "   \n",
    "    cosine_scores = []\n",
    "\n",
    "    # Make word 1 to be an equal length to word 2\n",
    "    word1_arr = [word1] * len(word2)\n",
    "\n",
    "    embeddings1 = model.encode(word1_arr)\n",
    "    embeddings2 = model.encode(word2)\n",
    "\n",
    "    # compute cosine_scores\n",
    "    cosine_matrix = util.cos_sim(embeddings1, embeddings2)\n",
    "    cosine_scores = cosine_matrix.diagonal()\n",
    "    \n",
    "    scores = cosine_scores.tolist()\n",
    "    \n",
    "    if logs == 1:\n",
    "        # create dataframe\n",
    "        df_wiki = pd.DataFrame(list(zip(word2, scores)),\n",
    "               columns =['target', 'weight'])\n",
    "        \n",
    "        # sort dataframe and reorganize index\n",
    "        df_wiki_org = df_wiki.sort_values(by=['weight'], ascending=False)\n",
    "        df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "        \n",
    "        print(df_wiki_org)\n",
    "        \n",
    "    # convert the tensor array to a list and return\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a2fa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    target    weight\n",
      "0    Karma  0.421835\n",
      "1     Pará  0.335476\n",
      "2      Fur  0.328533\n",
      "3  Feather  0.328417\n",
      "4   Bridge  0.317793\n"
     ]
    }
   ],
   "source": [
    "# for the following\n",
    "# start_article = 'Amazon River'\n",
    "# end_article = 'Emotion'\n",
    "\n",
    "word1 = 'Emotion'\n",
    "word2 = ['Karma', 'Pará', 'Fur', 'Feather', 'Bridge']\n",
    "model = \"L6\"\n",
    "scores = wordScore(word1, word2, model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6590ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playWikiGame(head, final, model):\n",
    "    steps = 0\n",
    "    pages = [head]\n",
    "    pages_visited = []\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    # forces the words to be the same case\n",
    "    while not str.lower(head) == str.lower(final):\n",
    "        result_links = getLinksFromTextBS(head)\n",
    "        len_target = len(final)\n",
    "\n",
    "        scores = wordScore(final, result_links, model)    \n",
    "        \n",
    "        # create dataframe\n",
    "        df_wiki = pd.DataFrame(list(zip(result_links, scores)),\n",
    "               columns =['target', 'weight'])\n",
    "        \n",
    "        # sort dataframe and reorganize index\n",
    "        df_wiki_org = df_wiki.sort_values(by=['weight'], ascending=False)\n",
    "        df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "        \n",
    "        # if the page has already been searched, drop it from the current links\n",
    "        while df_wiki_org['target'][0] in pages:\n",
    "            df_wiki_org = df_wiki_org.drop(labels=0, axis=0)\n",
    "            df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "            \n",
    "        head = df_wiki_org['target'][0]\n",
    "        pages.append(head)\n",
    "        print(head)\n",
    "        steps = steps + 1\n",
    "        pages_visited.append(head)\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    print(model + \" took \" + str(toc - tic) + \" seconds to complete\")\n",
    "    return steps, pages_visited, str(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054a2341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kenya Colony\n",
      "Colony\n",
      "Antarctica\n",
      "Life on Mars\n",
      "Mars\n",
      "L6 took 10.252214799999999 seconds to complete\n",
      "L6: It took 5 links to get from Horsepower to Mars\n"
     ]
    }
   ],
   "source": [
    "start_article = 'Horsepower'\n",
    "end_article = 'Mars'\n",
    "\n",
    "steps_l6, pages_visited_l6, time_l6 = playWikiGame(start_article, end_article, 'L6')\n",
    "print(\"L6: It took \" + str(steps_l6) + \" links to get from \" + start_article + \" to \" + end_article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae87ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running MiniLM-L6\n",
      "Karma\n",
      "Psychoanalysis\n",
      "Regressive emotionality\n",
      "Emotion\n",
      "L6 took 6.593670499999998 seconds to complete\n",
      "L6: It took 4 links to get from Amazon River to Emotion\n",
      "\n",
      "Running MiniLM-L12\n",
      "Karma\n",
      "Oceanic feeling\n",
      "Positive psychology\n",
      "Positive emotions\n",
      "Emotions\n",
      "Category:Emotions\n",
      "Category:Emotion\n",
      "Emotion\n",
      "L12 took 12.623686600000003 seconds to complete\n",
      "L12: It took 8 links to get from Amazon River to Emotion\n",
      "\n",
      "Running MPNet\n",
      "Karma\n",
      "Spirituality\n",
      "Happiness\n",
      "Emotion\n",
      "microsoftNet took 25.5028529 seconds to complete\n",
      "Microsoft: It took 4 links to get from Amazon River to Emotion\n",
      "\n",
      "Running Bert\n",
      "Sediment\n",
      "Concretion\n",
      "Lithification\n",
      "Pressure\n",
      "Force\n",
      "Motion\n",
      "Vibration\n",
      "Resonance\n",
      "Attenuation\n",
      "Energy\n",
      "Friction\n",
      "Atmosphere\n",
      "Terrain\n",
      "Fluvial\n",
      "Flow velocity\n",
      "Velocity\n",
      "Area\n",
      "Shapes\n",
      "Material\n",
      "Forces\n",
      "Dynamic pressure\n",
      "Irrotational flow\n",
      "Inviscid flow\n",
      "Fluid\n",
      "Matter\n",
      "Mass\n",
      "Momentum\n",
      "Body force\n",
      "Convective\n",
      "Fluids\n",
      "Droplets\n",
      "Liquid\n",
      "Body fluid\n",
      "Interstitial fluid\n",
      "Serous fluid\n",
      "Excretion\n",
      "Water\n",
      "Volatiles\n",
      "Surface tension\n",
      "Internal pressure\n",
      "Internal energy\n",
      "Extensive variable\n",
      "Specific volume\n",
      "Ratio\n",
      "Consequent\n",
      "Proposition\n",
      "Belief\n",
      "Emotion\n",
      "bert took 167.07772559999998 seconds to complete\n",
      "Bert: It took 48 links to get from Amazon River to Emotion\n",
      "\n",
      "Running Roberta\n",
      "Evangelism\n",
      "Passion Conferences\n"
     ]
    }
   ],
   "source": [
    "# Identify Starting Page\n",
    "start_article = 'Amazon River'\n",
    "end_article = 'Emotion'\n",
    "\n",
    "current_article = start_article\n",
    "\n",
    "print(\"\\nRunning MiniLM-L6\")\n",
    "steps_l6, pages_visited_l6, time_l6 = playWikiGame(start_article, end_article, 'L6')\n",
    "print(\"L6: It took \" + str(steps_l6) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning MiniLM-L12\")\n",
    "steps_l12, pages_visited_l12, time_l12 = playWikiGame(start_article, end_article, 'L12')\n",
    "print(\"L12: It took \" + str(steps_l12) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning MPNet\")\n",
    "steps_micro, pages_visited_micro, time_micro = playWikiGame(start_article, end_article, 'microsoftNet')\n",
    "print(\"Microsoft: It took \" + str(steps_micro) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning Bert\")\n",
    "steps_bert, pages_visited_bert, time_bert = playWikiGame(start_article, end_article, 'bert')\n",
    "print(\"Bert: It took \" + str(steps_bert) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning Roberta\")\n",
    "steps_roberta, pages_visited, time_roberta = playWikiGame(start_article, end_article, 'roberta')\n",
    "print(\"Roberta: It took \" + str(steps_roberta) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nL6:        It took \" + str(steps_l6) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_l6)\n",
    "print(\"L12:       It took \" + str(steps_l12) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_l12)\n",
    "print(\"Microsoft: It took \" + str(steps_micro) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_micro)\n",
    "print(\"Bert:      It took \" + str(steps_bert) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_bert)\n",
    "print(\"Roberta:   It took \" + str(steps_roberta) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_roberta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
