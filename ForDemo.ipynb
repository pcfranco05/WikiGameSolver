{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd41da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_article = 'Amazon River'\n",
    "end_article = 'Emotion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80562fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinksFromTextBS(start_article):\n",
    "    page_titles = []\n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'parse',\n",
    "        'page': start_article,\n",
    "        'format': 'json',\n",
    "        'prop': 'text',\n",
    "        'redirects': ''\n",
    "    }\n",
    "\n",
    "    filter_sections = ['See also',\n",
    "                       'References',\n",
    "                       'External links',\n",
    "                       'Further reading',\n",
    "                       'Notes']\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "\n",
    "    raw_html = data['parse']['text']['*']\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    # Get all the section names\n",
    "    allSections = soup.find_all(class_='mw-headline')\n",
    "\n",
    "    sectionNames = []\n",
    "    for section in allSections:\n",
    "        sectionNames.append(section.get_text())\n",
    "\n",
    "    sectionNames = [x for x in sectionNames if x not in filter_sections]\n",
    "\n",
    "    # Get links from summary section up to the Table of Contents\n",
    "    target = soup.find(class_='mw-parser-output')\n",
    "    if target:\n",
    "        for sib in target.find_all_next():\n",
    "            # Only look in current section, end if hitting next section\n",
    "            # print(sib)\n",
    "            if sib.name == \"h2\":\n",
    "                break\n",
    "            elif 'title' in sib.attrs and 'Edit this' in sib.attrs['title']:\n",
    "                # Don't include the hrefs to edit the pages\n",
    "                continue\n",
    "            else:\n",
    "                # Check if href contains internal /wiki/ path\n",
    "                check1 = 'href' in sib.attrs and \\\n",
    "                         '/wiki/' in sib.attrs['href']\n",
    "                # Check if tag contains class mw-redirect\n",
    "                check2 = 'class' in sib.attrs and \\\n",
    "                         'mw-redirect' in sib.attrs['class']\n",
    "                if (check1 or check2) and 'title' in sib.attrs and 'wiktionary' not in sib.attrs['title']:\n",
    "                    page_titles.append(sib.attrs['title'])\n",
    "\n",
    "    # Get all the links from each of the relevant sections\n",
    "    for thisSection in sectionNames:\n",
    "        # print('==--------' + thisSection + '--------==')\n",
    "        target = soup.find(class_='mw-headline', id=thisSection.replace(' ', '_'))\n",
    "        if target:\n",
    "            for sib in target.find_all_next():\n",
    "                # Only look in current section, end if hitting next section\n",
    "                # print(sib)\n",
    "                if sib.name == \"h2\":\n",
    "                    break\n",
    "                else:\n",
    "                    # Check if href contains internal /wiki/ path\n",
    "                    check1 = 'href' in sib.attrs and \\\n",
    "                    '/wiki/' in sib.attrs['href']\n",
    "                    # Check if tag contains class mw-redirect\n",
    "                    check2 = 'class' in sib.attrs and \\\n",
    "                        'mw-redirect' in sib.attrs['class']\n",
    "                    if (check1 or check2) and 'title' in sib.attrs:\n",
    "                        page_titles.append(sib.attrs['title'])\n",
    "    # Unique pages only\n",
    "    page_titles = list(set(page_titles))\n",
    "    return page_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe434a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gurupá', 'Jupati River', 'Caqueta River', 'Distributary', 'Timeline of Amazon history', 'Undular bore', 'Belém', 'Brazilian Institute of Geography and Statistics', 'Solimões River', 'Nevado Mismi', 'Richard Spruce', 'Curuçá', 'Matapi River', 'Solimões', 'Amazon Delta', 'Puerto Francisco de Orellana', 'Negro River (Amazon)', 'Arequipa', 'Lawriqucha', 'Tapajós', 'Nazca Plate', 'Spanish Empire', 'Pedro II of Brazil', 'Category:Trees of the Amazon', 'Andean civilizations', 'Paru River', 'Scythians', 'Guaporé River', 'Branco River', 'Hesychius of Alexandria', 'Indigenous peoples of Brazil', 'Irineu Evangelista de Sousa', 'Hamza River', 'Quito', 'Feather', 'Ucayali', 'Evangelism', 'Ticuna', 'Santarém, Pará', 'Tapajós River', 'Spanish conquistador', 'Floodplain', 'Acará River', 'Natural rubber', 'Vicente Yáñez Pinzón', 'Guayana Shield', 'Confluence', 'Cacao bean', 'Araguari River (Amapá)', 'Chambira River', 'River bifurcation', 'Uatumã River', 'Resin', 'Alphaproteobacteria', 'Pleistocene', 'Amazons', 'Orinoco River', 'Anavilhanas National Park', 'Getúlio Vargas', 'Pará', 'Headwaters', 'Miocene', 'Scholarly peer review', 'Atlantic Ocean', 'La Canela', 'Neon tetra', 'Amapá', 'Arowana', 'Kumakuma', 'Sarmatians', 'Terrace (agriculture)', 'Cinnamon', '5th parallel north', 'Malaria', 'Manaus', 'Upper Amazon', 'Electric eel', 'Guamá River', 'Muisca Confederation', 'Arequipa Region', 'Leticia, Amazonas', 'Lumber', 'James S. Olson', 'Biodiversity', 'Hammock', 'Teles Pires', 'Freshwater fish', 'Arapaima', 's:1911 Encyclopædia Britannica/Amazon', 'Main stem', 'Coca River', 'Porto Velho', 'Huánuco', 'Brazil', 'Iquitos', 'Ecosystem collapse', 'Sea level', 'Freshwater', 'Bandeirante', 'Actinomycetota', 'List of butterflies of the Amazon River basin and the Andes', 'Caiman', 'Paracauari River', 'Amazon rainforest', 'List of rivers by discharge', 'Gymnotiformes', 'River dolphin', 'Encyclopædia Britannica', 'Juruá River', 'Trombetas River', 'Ancient Greece', 'Javary River', 'Giant otter', 'Urubu River (Amazonas)', 'Green anaconda', 'Piorini River', 'Spanish language', 'Glacial period', 'Óbidos, Pará', 'Category:Birds of the Amazon Basin', 'Tucuxi', 'Coari River', 'Guajará River (Amazon)', 'Curuá River (Amazon River tributary)', 'Canumã River', 'Conquistador', 'Itacoatiara, Amazonas', 'Vaupés River', 'Animal echolocation', 'Caquetá River', 'Gondwana', 'Biodiversity of Colombia', 'Colombia', 'Steamboat', 'Ferry', 'Silver arowana', 'Tidal bore', 'Almeirim, Pará', 'Madre de Dios River', 'Anaconda', 'Bull shark', 'Tambo River (Peru)', 'Salinity', 'Lima', 'Dry season', 'Pracuúba', 'List of largest snakes', 'Iranian peoples', 'Pastaza River', 'Amazon delta', 'Wikipedia:Please clarify', 'Juruá', 'Peru', 'Paraná do Urariá', 'The Daily Telegraph', 'Category:Flora of the Amazon', 'Osteoglossum bicirrhosum', 'NASA', 'River mouth', 'Jandiatuba River', 'Santarém, Brazil', 'Urarina', 'Uarini River', 'Marajoara culture', 'Tefé River', 'BR-319', 'Pongo de Manseriche', 'Iquitos, Peru', 'Juruena River', 'Estuary', 'Óbidos, Brazil', 'Carl Friedrich Philipp von Martius', 'Ucayali River', 'Guainía River', 'Amacayacu National Park', 'Cabanagem', 'Discharge (hydrology)', 'Allpahuayo-Mishana National Reserve', 'University of Chicago', 'Candirú', 'Enlarge', 'Terra preta', 'Iriri River', 'Wet season', 'Coastline paradox', 'Area (journal)', 'Brazil nut', 'Badajós River', 'Lake Junin', 'Parintins', 'Igapó-Açu River', 'International Scale of River Difficulty', 'Nhamundá River', 'National Geographic', 'São Paulo', 'Royal Geographical Society', 'Karma', 'Muisca economy', 'Rio Preto da Eva', 'Nauta', 'Tabatinga', 'Arari River', 'Georg von Langsdorff', 'Amazon river dolphin', 'Anajás River', 'Putumayo River', 'Archaeologist', 'Munduruku', 'Potamotrygonidae', 'Help:IPA/English', 'Itaya River', 'Lake Titicaca', 'National Institute for Space Research', 'Marañón River', 'Mantaro River', 'Tributaries', 'Johann Baptist von Spix', 'Purus River', 'Trombetas', 'South America', 'Santarem, Brazil', 'Amazon Theatre', 'Macapá', 'Amazonas (Brazilian state)', 'British English', 'Samuel Fritz', 'South American Plate', 'Lope de Aguirre', 'Grão-Pará Province', 'Vila Nova River', 'Tocantins River', 'Alfred Russel Wallace', 'Betaproteobacteria', 'Greek mythology', 'Pacific Ocean', 'Tupi language', 'Napo River', 'Bay', 'Morona', 'Kawahíb people', 'Nanay River', 'Ecuador', 'El Dorado', 'Pardo', 'Manacapuru', 'Piranha', 'Alexander von Humboldt', 'Tonantins River', 'Meeting of Waters', 'Monte Alegre, Pará', 'Typhus', 'Tambaqui', 'Jutai River', 'Dolphin', 'Curuá Una River', 'International Union for Conservation of Nature', 'Public domain', 'Gammaproteobacteria', 'Ethnonym', 'Agriculture', 'Amazon basin', 'Japurá River', 'Neon Tetra', 'List of river systems by length', 'Jari River', 'Portuguese language', 'Rubber boom', 'Encyclopædia Britannica Eleventh Edition', 'Sanskrit', 'Amazônia National Park', 'Araguaia River', 'American English', 'Trans-Amazonian Highway', 'Apurimac River', 'Tigre River', 'Casiquiare canal', 'Cretaceous', 'Huallaga River', 'António Raposo Tavares', 'Apurímac River', 'Aguarico River', 'Category:Fauna of the Amazon', 'Colonization', 'Freshwater Angelfish', 'Marajó', 'Indo-Iranian languages', 'Jutaí River', 'Gonzalo Pizarro', 'Wikipedia:Manual of Style/Words to watch', 'Freshwater swamp forest', 'Stream channel', '20th parallel south', 'Tefé', 'Inca Empire', 'Formative stage', 'Madeira River', 'Urubamba River', 'George Earl Church', 'Sandstone', 'Breves, Pará', 'Thermoproteota', 'Catfish', 'Charles Marie de La Condamine', 'Manacapuru River', 'Draft (hull)', 'Pororoca', 'White Brazilian', 'Pedreira River', 'Mercantile', 'Carhuasanta', 'Pedro Teixeira', 'Anabranch', 'Purús River', 'Henry Walter Bates', 'Tributary', 'Mamiá River (Amazonas)', 'Chiefdom', 'Characin', 'Francisco de Orellana', 'Drainage basin', 'Source of the Amazon River', 'Pará River', 'Sediment', 'Fur', 'Atuá River', 'Neotropical fishes', 'The Naturalist on the River Amazons', 'Bridge', 'Rio Negro (Amazon)', 'Andes', 'Orinoco', 'List of rivers by length', 'Trichomycteridae', 'Andes Mountains', 'Nile', 'Manaus Iranduba Bridge', 'Boto', 'Metagenomics', 'Amazonian manatee', 'Tectonic uplift', 'Maicuru River', 'Xingu River']\n"
     ]
    }
   ],
   "source": [
    "# for the following\n",
    "# start_article = 'Amazon River'\n",
    "# end_article = 'Emotion'\n",
    "\n",
    "links = getLinksFromTextBS('Amazon River')\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3269081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Word Similarity Check\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# wordScore takes in the list of words from a wikipedia link and the name of the goal page.\n",
    "def wordScore(word1, word2, model, logs=0):\n",
    "    if model == 'bert':\n",
    "        model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    elif model == 'roberta':\n",
    "        model = SentenceTransformer('all-distilroberta-v1')\n",
    "    elif model == 'microsoftNet':\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    elif model == 'L12':\n",
    "        model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    elif model == 'L6':\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "   \n",
    "    cosine_scores = []\n",
    "\n",
    "    # Make word 1 to be an equal length to word 2\n",
    "    word1_arr = [word1] * len(word2)\n",
    "\n",
    "    embeddings1 = model.encode(word1_arr)\n",
    "    embeddings2 = model.encode(word2)\n",
    "\n",
    "    # compute cosine_scores\n",
    "    cosine_matrix = util.cos_sim(embeddings1, embeddings2)\n",
    "    cosine_scores = cosine_matrix.diagonal()\n",
    "    \n",
    "    scores = cosine_scores.tolist()\n",
    "    \n",
    "    if logs == 1:\n",
    "        # create dataframe\n",
    "        df_wiki = pd.DataFrame(list(zip(word2, scores)),\n",
    "               columns =['target', 'weight'])\n",
    "        \n",
    "        # sort dataframe and reorganize index\n",
    "        df_wiki_org = df_wiki.sort_values(by=['weight'], ascending=False)\n",
    "        df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "        \n",
    "        print(df_wiki_org)\n",
    "        \n",
    "    # convert the tensor array to a list and return\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a2fa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    target    weight\n",
      "0    Karma  0.421835\n",
      "1     Pará  0.335476\n",
      "2      Fur  0.328533\n",
      "3  Feather  0.328417\n",
      "4   Bridge  0.317793\n"
     ]
    }
   ],
   "source": [
    "# for the following\n",
    "# start_article = 'Amazon River'\n",
    "# end_article = 'Emotion'\n",
    "\n",
    "word1 = 'Emotion'\n",
    "word2 = ['Karma', 'Pará', 'Fur', 'Feather', 'Bridge']\n",
    "model = \"L6\"\n",
    "scores = wordScore(word1, word2, model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b596dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check page views\n",
    "def getPageViews(start_article):\n",
    "    start_article_formatted = start_article.replace(\" \",\"_\")\n",
    "    start_article_formatted\n",
    "    links = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/{}/monthly/20221101/20221201\".format(start_article_formatted)\n",
    "\n",
    "    header={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(links, headers=header).json()\n",
    "    page_views = response['items'][0]['views']\n",
    "    return page_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b7f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60968\n"
     ]
    }
   ],
   "source": [
    "print(getPageViews('Karma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6590ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playWikiGame(head, final, model):\n",
    "    steps = 0\n",
    "    pages = [head]\n",
    "    pages_visited = []\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "    \n",
    "    # forces the words to be the same case\n",
    "    while not str.lower(head) == str.lower(final):\n",
    "        result_links = getLinksFromTextBS(head)\n",
    "        len_target = len(final)\n",
    "\n",
    "        scores = wordScore(final, result_links, model)    \n",
    "        \n",
    "        # create dataframe\n",
    "        df_wiki = pd.DataFrame(list(zip(result_links, scores)),\n",
    "               columns =['target', 'weight'])\n",
    "        \n",
    "        # sort dataframe and reorganize index\n",
    "        df_wiki_org = df_wiki.sort_values(by=['weight'], ascending=False)\n",
    "        df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "        \n",
    "        # if the page has already been searched, drop it from the current links\n",
    "        while df_wiki_org['target'][0] in pages:\n",
    "            df_wiki_org = df_wiki_org.drop(labels=0, axis=0)\n",
    "            df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "            \n",
    "        head = df_wiki_org['target'][0]\n",
    "        pages.append(head)\n",
    "        print(head)\n",
    "        steps = steps + 1\n",
    "        pages_visited.append(head)\n",
    "    \n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    print(model + \" took \" + str(toc - tic) + \" seconds to complete\")\n",
    "    return steps, pages_visited, str(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bae87ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running MiniLM-L6\n",
      "Karma\n",
      "Psychoanalysis\n",
      "Regressive emotionality\n",
      "Emotion\n",
      "L6 took 6.635767600000008 seconds to complete\n",
      "L6: It took 4 links to get from Amazon River to Emotion\n",
      "\n",
      "Running MiniLM-L12\n",
      "Karma\n",
      "Oceanic feeling\n",
      "Positive psychology\n",
      "Positive emotions\n",
      "Emotions\n",
      "Category:Emotions\n",
      "Category:Emotion\n",
      "Emotion\n",
      "L12 took 12.75224030000004 seconds to complete\n",
      "L12: It took 8 links to get from Amazon River to Emotion\n",
      "\n",
      "Running MPNet\n",
      "Karma\n",
      "Spirituality\n",
      "Happiness\n",
      "Emotion\n",
      "microsoftNet took 25.109606500000154 seconds to complete\n",
      "Microsoft: It took 4 links to get from Amazon River to Emotion\n",
      "\n",
      "Running Bert\n",
      "Sediment\n",
      "Concretion\n",
      "Lithification\n",
      "Pressure\n",
      "Force\n",
      "Motion\n",
      "Vibration\n",
      "Resonance\n",
      "Attenuation\n",
      "Energy\n",
      "Friction\n",
      "Atmosphere\n",
      "Terrain\n",
      "Fluvial\n",
      "Flow velocity\n",
      "Velocity\n",
      "Area\n",
      "Shapes\n",
      "Material\n",
      "Forces\n",
      "Dynamic pressure\n",
      "Irrotational flow\n",
      "Inviscid flow\n",
      "Fluid\n",
      "Matter\n",
      "Mass\n",
      "Momentum\n",
      "Body force\n",
      "Convective\n",
      "Fluids\n",
      "Droplets\n",
      "Liquid\n",
      "Body fluid\n",
      "Interstitial fluid\n",
      "Serous fluid\n",
      "Excretion\n",
      "Water\n",
      "Volatiles\n",
      "Surface tension\n",
      "Internal pressure\n",
      "Internal energy\n",
      "Extensive variable\n",
      "Specific volume\n",
      "Ratio\n",
      "Consequent\n",
      "Proposition\n",
      "Belief\n",
      "Emotion\n",
      "bert took 160.36097519999998 seconds to complete\n",
      "Bert: It took 48 links to get from Amazon River to Emotion\n",
      "\n",
      "Running Roberta\n",
      "Evangelism\n",
      "Passion Conferences\n",
      "Passion (worship band)\n",
      "Passion: Awakening\n",
      "Passion: Here for You\n",
      "Passion: White Flag\n",
      "Passion: Let the Future Begin\n",
      "Passion: Take It All\n",
      "Passion: Even So Come\n",
      "Passion: Salvation's Tide Is Rising\n",
      "Music genre\n",
      "Rhythm\n",
      "Mensural level\n",
      "Music\n",
      "Emotions\n",
      "Emotion (disambiguation)\n",
      "Emotion\n",
      "roberta took 32.935931900000014 seconds to complete\n",
      "Roberta: It took 17 links to get from Amazon River to Emotion\n",
      "\n",
      "L6:        It took 4 links to get from Amazon River to Emotion in 6.635767600000008\n",
      "L12:       It took 8 links to get from Amazon River to Emotion in 12.75224030000004\n",
      "Microsoft: It took 4 links to get from Amazon River to Emotion in 25.109606500000154\n",
      "Bert:      It took 48 links to get from Amazon River to Emotion in 160.36097519999998\n",
      "Roberta:   It took 17 links to get from Amazon River to Emotion in 32.935931900000014\n"
     ]
    }
   ],
   "source": [
    "# Identify Starting Page\n",
    "start_article = 'Amazon River'\n",
    "end_article = 'Emotion'\n",
    "\n",
    "current_article = start_article\n",
    "\n",
    "print(\"\\nRunning MiniLM-L6\")\n",
    "steps_l6, pages_visited_l6, time_l6 = playWikiGame(start_article, end_article, 'L6')\n",
    "print(\"L6: It took \" + str(steps_l6) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning MiniLM-L12\")\n",
    "steps_l12, pages_visited_l12, time_l12 = playWikiGame(start_article, end_article, 'L12')\n",
    "print(\"L12: It took \" + str(steps_l12) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning MPNet\")\n",
    "steps_micro, pages_visited_micro, time_micro = playWikiGame(start_article, end_article, 'microsoftNet')\n",
    "print(\"Microsoft: It took \" + str(steps_micro) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning Bert\")\n",
    "steps_bert, pages_visited_bert, time_bert = playWikiGame(start_article, end_article, 'bert')\n",
    "print(\"Bert: It took \" + str(steps_bert) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning Roberta\")\n",
    "steps_roberta, pages_visited, time_roberta = playWikiGame(start_article, end_article, 'roberta')\n",
    "print(\"Roberta: It took \" + str(steps_roberta) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nL6:        It took \" + str(steps_l6) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_l6)\n",
    "print(\"L12:       It took \" + str(steps_l12) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_l12)\n",
    "print(\"Microsoft: It took \" + str(steps_micro) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_micro)\n",
    "print(\"Bert:      It took \" + str(steps_bert) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_bert)\n",
    "print(\"Roberta:   It took \" + str(steps_roberta) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e84ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
